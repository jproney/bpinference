---
title: "one-type-bp-inference"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{one-type-bp-inference}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

In this vignette, we consider the problem of inferring the birth and death rate for a one-type Markov branching process using simulated data. This process consists of a population of cells which split into two cells at rate `b` and die at rate `d`, as illustrated by the following diagram:

First we load the `bpinference` package.

```{r setup}
library(ggplot2)
devtools::load_all()
library(rstan)
```

Next, we'll define the structure of our model. This requires three main ingredients:

* A matrix of possible birth events. A birth event occurs when a parent cell dies and gives rise to some number of offpring. The birth events matrix (defined as `e_mat` in the code) contains one row for each possible birth event, and one column for each cell type. The entry `e_mat[i,j]` represents how many cells of type `j` are born when birth event `i` occurs. (Note that `e_mat` must always be a **matrix**, even when there is only one cell type).
* A parents vector. The parents vector (defined as `p_vec`) has one entry for each birth event. The entry `p_vec[i]` contains the cell type of the parent which dies when birth even `i` occurs.
* A vector of functional dependencies. Sometimes, we may want the rates of various birth events in our model to vary as some function of an environmental variance. To acoomplish this, we specify functional dependencies as strings using a specific syntax. In essence, this syntax expects parameter number `i` to be referenced as `'c[i]'`, and dependent variable number `j` to be references as `'x[j]'`. For full detail on this syntax, see the man page for `bp_model`. In this particular case, we have no dependent varianbles. Therefore we only need two parameters, one for birth an one for death. 

For our one-type birth-death model, this looks like the following:

```{r}
e_mat <-  matrix(c(2,0),ncol=1)
p_vec <- c(1, 1)
func_deps <- c('c[1]','c[2]')
n_params <- 2
n_deps <- 0
mod = bp_model(e_mat, p_vec, func_deps, n_params, n_deps)
```

We now have a `bp_model` object which we can use for simulation and inference. For fun, lets pick some parameter values and simulate a lot of data:

```{r}
simulation_params <- c(0.25, 0.10)
z0 <- c(1000) # initial population vector
times <- 1
simulation_dat <- bpsims(mod, simulation_params, z0, times, 5000)
ggplot(simulation_dat, aes(x=pop)) + geom_histogram(binwidth = 5, color="black",fill="white") 
```

The data look very normal, which is what we should expect for a large starting population `z0 = 1000`. Are the mean and variance of this distribution similar to what we should expect analytically? We can check using the `calculat_moments` function:

```{r}
mom <- calculate_moments(e_mat, p_vec, simulation_params, z0, 1)

# should be in close agreement
print(paste("true mean:", mom$mu_mat, "sample mean:", mean(simulation_dat$pop), sep=" "))
print(paste("true variance:", mom$sigma_mat, "sample variance:", var(simulation_dat$pop), sep=" "))
```

Now that we've confirmed our asymtotic normality, let's do some inference. To make this more realistic, let's simulate a new dataset with 5 timepoints and 50 replications.

```{r}
simulation_params <- c(0.25, 0.10)
z0 <- c(1000) # initial population vector
times <- seq(0,5)
simulation_dat <- bpsims(mod, simulation_params, z0, times, 50)
```

To do inference, we need to generate a model that will run in Stan. Stan is a flexible probabilistic programming language that allows for us to sample from arbitrary Bayesian posterior distributions. But before we can create a stan model, we have to define prior distributions which encode our current thoughts about the parameters of the model. For now we'll just let both parameters vary uniformly from 0 to 2.

```{r}
priors <- rep(list(list(name="normal",params=c(0, 2), bounds=c(0,2))),n_params)
generate(mod, priors, "one_type_bp_inference.stan")
```

The `generate` function automatically creates a stan model for us to sample from. There should now be a file named "one_type_bp_inference.stan" in the working directory with a lot of code. This code essentially just computes the likelihood for our model given some input data. Feel free to read the generated code, and even modify it if you have a good reason.

The last thing we need to do is convert our simulation data into a form that the Stan model will understand. We can do this using the `stan_data_from_simulation` function. At the same time, we'll create some unformly random inital values for Stan's MCMC algorithm.

```{r}
dat <- stan_data_from_simulation(simulation_dat, mod)
ranges <- matrix(rep(c(0,1),n_params), n_params,2,byrow = T)
init <- uniform_initialize(ranges, 4)
```

With that, we're ready to sample! The following code will tell Stan to sample from the posterior using 4 Markov chains, collecting 2000 samples from each:

```{r}
options(mc.cores = parallel::detectCores())
stan_mod <- rstan::stan_model(file = "one_type_bp_inference.stan")
fit_data <- rstan::sampling(stan_mod, data = dat, control = list(adapt_delta = 0.95), chains = 4, refresh = 50, init =init, iter=3000,warmup=1000)
samples <- data.frame(rstan::extract(fit_data))
```

Just how good was our estimation? Let's check:

```{r}
print(paste("true birth:", .25, "estimated birth:", mean(samples$Theta1), sep=" "))
print(paste("true death:", .10, "estimated death:", mean(samples$Theta2), sep=" "))
```

In addition to just using point estimates of the posterior means, we can view our entire posterior distributions as well:

```{r}
ggplot(samples, aes(x = Theta1)) + geom_histogram(binwidth = .004, color='black', fill='white')
ggplot(samples, aes(x = Theta2)) + geom_histogram(binwidth = .004, color='black', fill='white')
```